{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdion-ken/mbti-text-analysis/blob/main/notebook/MBTI_Text_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c22fb48"
      },
      "source": [
        "# Psychology of Social Media Behavior and Online Personas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f80fa8b5"
      },
      "source": [
        "## 1. Overview and Goal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfpG5Ve4misE"
      },
      "source": [
        "This project explores how **personality traits shape language patterns in online self expression**.\n",
        "\n",
        "Using the **MBTI 500 Personality Types Dataset**, the study analyzes how distinct personality categories correspond to unique linguistic signatures.\n",
        "\n",
        "Rather than focusing on tone or syntax, the project emphasizes **quantitative text analysis and machine learning classification.**\n",
        "\n",
        "By examining the relationship between MBTI dimensions and text features, it uncovers whether personality differences can be inferred from the words people use, even after text standardization.\n",
        "\n",
        "Specifically, this study also aims to:\n",
        "- Identify **linguistic features** that distinguish personality types.\n",
        "- Train and evaluate a **text classification model** capable of predicting MBTI traits from language data.\n",
        "- Visualize the relationships between **personality dimensions** and **dominant lexical patterns** within the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88de1c10"
      },
      "source": [
        "## 2. Dataset Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2_a0KtYm9Br"
      },
      "source": [
        "**Dataset:** MBTI 500 Personality Types Dataset  \n",
        "**Source:** [Kaggle – MBTI 500 Dataset](https://www.kaggle.com/datasets/zeyadkhalid/mbti-personality-types-500-dataset/data) (Zeyad Khalid, 2021)  \n",
        "\n",
        "This dataset contains approximately 106,000 preprocessed records of user posts paired with MBTI personality types, stored in `mbti_500.csv`\n",
        "\n",
        "**Structure:**\n",
        "- `type` → 16 MBTI personality type (e.g. INTJ, ENFP)  \n",
        "- `posts` → 500 word text sample from Reddit/PersonalityCafe  \n",
        "\n",
        "**Timeframe:** Posts originally gathered between 2016–2019.  Cleaned in 2021.  \n",
        "\n",
        "Since the text has been preprocessed punctuation, stopwords, and URLs removed, and all samples standardized to equal length, this dataset is particularly well suited for **machine learning classification tasks.**\n",
        "\n",
        "It provides a large dataset ideal for identifying **lexical patterns** and **predictive linguistic features** associated with personality types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd0f40ec"
      },
      "source": [
        "## 3. Methodology and Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX0bscj4n3qy"
      },
      "source": [
        "### Step 1: Data Access and Library Import\n",
        "Import core libraries and download the MBTI dataset directly for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "898cb67d"
      },
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning tools\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# For linguistic insights later\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from collections import Counter\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Display options\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "# Download MBTI dataset\n",
        "url = \"https://drive.google.com/uc?id=1KM-1SAIhKhLhdQawPKBoyR3vGBCTJPzV\"\n",
        "output = \"mbti_500.csv\"\n",
        "\n",
        "print(\"Downloading dataset...\")\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Load dataset\n",
        "try:\n",
        "    data = pd.read_csv(output)\n",
        "    print(f\"Dataset loaded successfully. Shape: {data.shape}\")\n",
        "    print(\"Columns:\", list(data.columns))\n",
        "except Exception as e:\n",
        "    print(\"Error loading dataset:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d59ea0e4"
      },
      "source": [
        "### Step 2: Data Cleaning and Feature Engineering\n",
        "\n",
        "Since the text in this dataset has already been lemmatized, standardized, and stripped of punctuation, stopwords, and URLs, it is well suited for machine learning based text classification.\n",
        "\n",
        "This stage focuses on preparing the dataset for modeling by ensuring data integrity and engineering structured features from the MBTI types.\n",
        "\n",
        "\n",
        "1. Remove duplicates and missing values to ensure dataset consistency.\n",
        "2. Verify text normalization, confirming that all entries are lowercase and alphabetic.\n",
        "3. Split MBTI personality types into four binary columns representing the individual psychological dimensions:\n",
        "*   Introversion (I) vs. Extraversion (E)\n",
        "*   Sensing (S) vs. Intuition (N)\n",
        "*   Thinking (T) vs. Feeling (F)\n",
        "*   Judging (J) vs. Perceiving (P)\n",
        "4. Prepare the processed dataset for vectorization and machine learning classification in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkH7fLJAv0hj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas(desc=\"Processing text features\")\n",
        "\n",
        "# Remove duplicates and missing values\n",
        "data.drop_duplicates(inplace=True)\n",
        "data.dropna(inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Split MBTI types into binary dimensions\n",
        "data[\"I_E\"] = data[\"type\"].apply(lambda x: 1 if \"I\" in x else 0)  # 1 = Introvert, 0 = Extrovert\n",
        "data[\"S_N\"] = data[\"type\"].apply(lambda x: 1 if \"S\" in x else 0)  # 1 = Sensing, 0 = Intuitive\n",
        "data[\"T_F\"] = data[\"type\"].apply(lambda x: 1 if \"T\" in x else 0)  # 1 = Thinking, 0 = Feeling\n",
        "data[\"J_P\"] = data[\"type\"].apply(lambda x: 1 if \"J\" in x else 0)  # 1 = Judging, 0 = Perceiving\n",
        "\n",
        "# Overview of class balance\n",
        "print(\"Data cleaning completed.\")\n",
        "print(f\"Dataset shape after cleaning: {data.shape}\")\n",
        "print(\"\\nPersonality type distribution:\")\n",
        "print(data['type'].value_counts().head(10))\n",
        "\n",
        "# Preview a few samples\n",
        "display(data[[\"type\", \"posts\"]].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIxtnw1yrOG8"
      },
      "source": [
        "### Step 3: Save Cleaned Dataset\n",
        "After verifying data integrity and preparing MBTI dimension columns, we export the cleaned dataset for reuse and modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwzG2kSN-nQI"
      },
      "outputs": [],
      "source": [
        "data.to_csv(\"clean_mbti_500.csv\", index=False)\n",
        "print(\"Cleaned dataset saved as clean_mbti_500.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0540785f"
      },
      "source": [
        "### Step 4: Analyze N-gram Frequencies\n",
        "\n",
        "This step explores the most common word patterns (unigrams, bigrams, trigrams) used by different personality types.\n",
        "\n",
        "By identifying frequent word combinations, we can infer stylistic and thematic tendencies in how individuals of varying MBTI types express themselves.\n",
        "\n",
        "Even though the dataset is preprocessed and standardized, word frequency patterns can still reveal meaningful **lexical preferences** (“feel like” for Feeling types vs. “think about” for Thinking types)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4aebf60"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned dataset\n",
        "data = pd.read_csv(\"clean_mbti_500.csv\")\n",
        "\n",
        "def get_top_ngram(corpus, n=None, num_words=10):\n",
        "    vec = CountVectorizer(stop_words='english', ngram_range=(n, n))\n",
        "    bag = vec.fit_transform(corpus)\n",
        "    sum_words = bag.sum(axis=0)\n",
        "    words_freq = [(word, int(sum_words[0, idx])) for word, idx in vec.vocabulary_.items()]\n",
        "    return sorted(words_freq, key=lambda x: x[1], reverse=True)[:num_words]\n",
        "\n",
        "ngram_analysis = {}\n",
        "\n",
        "# Use tqdm progress bar for each MBTI type\n",
        "for mbti_type in tqdm(data['type'].unique(), desc=\"Analyzing MBTI Types\"):\n",
        "    type_posts = data.loc[data['type'] == mbti_type, 'posts']\n",
        "    top_bigrams = get_top_ngram(type_posts, n=2, num_words=10)\n",
        "    top_trigrams = get_top_ngram(type_posts, n=3, num_words=10)\n",
        "    ngram_analysis[mbti_type] = {\n",
        "        'bigrams': top_bigrams,\n",
        "        'trigrams': top_trigrams\n",
        "    }\n",
        "\n",
        "# Display sample output\n",
        "for mbti_type in list(ngram_analysis.keys())[:3]:\n",
        "    print(f\"\\nTop N-grams for {mbti_type}:\")\n",
        "    print(\"  Bigrams:\", ngram_analysis[mbti_type]['bigrams'])\n",
        "    print(\"  Trigrams:\", ngram_analysis[mbti_type]['trigrams'])\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd1e37fd"
      },
      "source": [
        "### Step 5: Part of Speech Frequency Analysis\n",
        "\n",
        "The goal here is to quantify how each MBTI type uses different grammatical categories nouns, verbs, adjectives, pronouns, etc. using a representative sample of posts (~500 per MBTI type).\n",
        "\n",
        "*   Intuitives might use more adjectives/adverbs (abstract or descriptive language).\n",
        "*   Thinkers might favor nouns and verbs (analytical, direct phrasing).\n",
        "*   Feelers might retain more pronouns (relational focus).\n",
        "\n",
        "\n",
        "Approch:\n",
        "*   Apply spaCy to compute POS ratios per post.\n",
        "*   Aggregate ratios by MBTI type for comparative insights.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "903e4734"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load cleaned dataset\n",
        "data = pd.read_csv(\"clean_mbti_500.csv\")\n",
        "\n",
        "# Stratified sampling: ~500 posts per MBTI type\n",
        "sampled_data = data.groupby('type', group_keys=False).apply(lambda x: x.sample(min(len(x), 500), random_state=42)).reset_index(drop=True)\n",
        "print(f\"Sampled dataset shape: {sampled_data.shape}\")\n",
        "\n",
        "# Load spaCy model\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
        "\n",
        "# Define POS categories\n",
        "pos_map = {\n",
        "    \"NOUN\": [\"NOUN\", \"PROPN\"],\n",
        "    \"VERB\": [\"VERB\", \"AUX\"],\n",
        "    \"ADJ\": [\"ADJ\"],\n",
        "    \"ADV\": [\"ADV\"],\n",
        "    \"PRON\": [\"PRON\"]\n",
        "}\n",
        "\n",
        "# Function to compute POS ratios\n",
        "def get_pos_ratios(text):\n",
        "    doc = nlp(text)\n",
        "    counts = Counter()\n",
        "    for token in doc:\n",
        "        for pos_category, spacy_tags in pos_map.items():\n",
        "            if token.pos_ in spacy_tags:\n",
        "                counts[pos_category] += 1\n",
        "                break\n",
        "    total = sum(counts.values()) or 1\n",
        "    return {f\"{pos}_ratio\": counts[pos]/total for pos in pos_map.keys()}\n",
        "\n",
        "# Apply POS analysis to the sampled dataset\n",
        "tqdm.pandas(desc=\"POS analysis\")\n",
        "pos_features = sampled_data['posts'].progress_apply(get_pos_ratios)\n",
        "pos_df = pd.DataFrame(list(pos_features))\n",
        "\n",
        "# Combine results with MBTI types\n",
        "sampled_pos_data = pd.concat([sampled_data.reset_index(drop=True), pos_df], axis=1)\n",
        "\n",
        "# Aggregate by MBTI type\n",
        "pos_summary = sampled_pos_data.groupby(\"type\")[[\"NOUN_ratio\",\"VERB_ratio\",\"ADJ_ratio\",\"ADV_ratio\",\"PRON_ratio\"]].mean()\n",
        "print(\"POS Frequency Summary (by MBTI type):\")\n",
        "display(pos_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c4241ad"
      },
      "source": [
        "### Step 6: TF-IDF Vectorization + Simple Classifier\n",
        "\n",
        "Transforms the preprocessed text into a numerical representation using TF-IDF and train a simple classifier to predict MBTI personality traits. This serves as a proof of concept to show whether language patterns are predictive of personality types.\n",
        "\n",
        "\n",
        "\n",
        "*   Use TF-IDF to vectorize text while capturing term importance.\n",
        "*   Train a Logistic Regression model for multi-class classification.\n",
        "*   Evaluate using accuracy and confusion matrix for interpretability.\n",
        "*   Can optionally run on a subset for faster experimentation in Colab.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlMgsH9OJBT9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Load cleaned dataset\n",
        "data = pd.read_csv(\"clean_mbti_500.csv\")\n",
        "\n",
        "# Optional: use a subset for faster testing (like 5000 posts)\n",
        "# data = data.sample(5000, random_state=42)\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tqdm.pandas(desc=\"Vectorizing text\")\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(tqdm(data['posts']))\n",
        "\n",
        "# Target labels\n",
        "y = data['type']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Logistic Regression classifier\n",
        "clf = LogisticRegression(max_iter=500, multi_class='ovr', n_jobs=-1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Classifier Accuracy: {acc:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=data['type'].unique())\n",
        "cm_df = pd.DataFrame(cm, index=data['type'].unique(), columns=data['type'].unique())\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "display(cm_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08fca0d4"
      },
      "source": [
        "## 4. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GKdBjhXLFky"
      },
      "source": [
        "### MBTI Dimension Distribution\n",
        "\n",
        "*   See balance in I/E, S/N, T/F, J/P.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC3LAM8l-29x"
      },
      "outputs": [],
      "source": [
        "dimension_stats = pd.DataFrame({\n",
        "    \"Dimension\": [\"I/E\", \"S/N\", \"T/F\", \"J/P\"],\n",
        "    \"Majority\": [\n",
        "        \"Introvert\" if data[\"I_E\"].mean() > 0.5 else \"Extrovert\",\n",
        "        \"Sensing\" if data[\"S_N\"].mean() > 0.5 else \"Intuition\",\n",
        "        \"Thinking\" if data[\"T_F\"].mean() > 0.5 else \"Feeling\",\n",
        "        \"Judging\" if data[\"J_P\"].mean() > 0.5 else \"Perceiving\"\n",
        "    ],\n",
        "    \"Split\": [\n",
        "        f\"{data['I_E'].mean():.1%} / {1-data['I_E'].mean():.1%}\",\n",
        "        f\"{data['S_N'].mean():.1%} / {1-data['S_N'].mean():.1%}\",\n",
        "        f\"{data['T_F'].mean():.1%} / {1-data['T_F'].mean():.1%}\",\n",
        "        f\"{data['J_P'].mean():.1%} / {1-data['J_P'].mean():.1%}\"\n",
        "    ],\n",
        "    \"Balance_Score\": [\n",
        "        abs(data['I_E'].mean() - 0.5),\n",
        "        abs(data['S_N'].mean() - 0.5),\n",
        "        abs(data['T_F'].mean() - 0.5),\n",
        "        abs(data['J_P'].mean() - 0.5)\n",
        "    ]\n",
        "})\n",
        "print(\"\\nClass Balance by Dimension (0.0 = perfect balance, 0.5 = max imbalance):\")\n",
        "display(dimension_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMQp755KWQiB"
      },
      "source": [
        "The dataset shows varying levels of imbalance across MBTI dimensions. The I/E dimension leans heavily toward Introverts (76%), while the S/N dimension is extremely skewed toward Intuition (91%). T/F favors Thinking (65%), and J/P is relatively balanced. These imbalances may affect downstream classification performance and should be considered when evaluating model results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slad5zELLOIg"
      },
      "source": [
        "### Class Imbalance Analysis\n",
        "\n",
        "*   Quantify type imbalance\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti_XiZtRUiUn"
      },
      "outputs": [],
      "source": [
        "type_counts = data['type'].value_counts()\n",
        "print(f\"\\nMost common type: {type_counts.index[0]} ({type_counts.iloc[0]} posts)\")\n",
        "print(f\"Least common type: {type_counts.index[-1]} ({type_counts.iloc[-1]} posts)\")\n",
        "print(f\"Imbalance ratio: {type_counts.iloc[0] / type_counts.iloc[-1]:.1f}x\")\n",
        "print(f\"Types with <100 samples: {(type_counts < 100).sum()} out of 16\")\n",
        "print(f\"Types with <50 samples: {(type_counts < 50).sum()} out of 16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR-caNleWbSV"
      },
      "source": [
        "The dataset is highly imbalanced at the individual MBTI type level. INTP is the most represented type with ~25k posts, whereas ESFJ has only 181 posts, yielding an imbalance ratio of ~138x. Fortunately, all types have at least 50 posts, so none are completely missing, but this skew should be kept in mind for model training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hbt29MxUod9"
      },
      "source": [
        "### N-gram Diversity Analysis\n",
        "\n",
        "*   Summary stats for language patterns\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIRx06OrUtKn"
      },
      "outputs": [],
      "source": [
        "ngram_diversity = {}\n",
        "for mbti_type in ngram_analysis.keys():\n",
        "    bigrams = ngram_analysis[mbti_type]['bigrams']\n",
        "    if len(bigrams) > 0:\n",
        "        total_bigrams = sum([count for _, count in bigrams])\n",
        "        ngram_diversity[mbti_type] = {\n",
        "            'total_bigrams': total_bigrams,\n",
        "            'unique_in_top10': len(bigrams),\n",
        "            'top1_concentration': bigrams[0][1] / total_bigrams if total_bigrams > 0 else 0,\n",
        "            'most_common': bigrams[0][0]\n",
        "        }\n",
        "\n",
        "ngram_df = pd.DataFrame(ngram_diversity).T\n",
        "print(\"\\nN-gram Diversity by Type:\")\n",
        "print(\"(Higher concentration = more repetitive language)\")\n",
        "display(ngram_df.sort_values('top1_concentration', ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWZvu3HjWmVh"
      },
      "source": [
        "The N-gram analysis shows that while the top 10 bigrams are consistent across all types, the frequency of the most common bigram varies. ‘Feel like’ dominates for most types, reflecting a common informal phrasing in online posts. ESTP shows the lowest concentration of its top bigram, suggesting more lexical diversity, whereas INFP and INFJ have higher concentrations, indicating more repetitive language patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3n1-bxBOwYC"
      },
      "source": [
        "### TF-IDF Feature Importance\n",
        "\n",
        "*   Identify which words/bigrams are most predictive of each type.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyJla9SoO2UY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"\\nTop 10 TF-IDF Features by MBTI Type:\\n\")\n",
        "for idx, mbti_type in enumerate(clf.classes_):\n",
        "    coefs = clf.coef_[idx]\n",
        "    top_idx = np.argsort(coefs)[-10:]\n",
        "    top_features = np.array(vectorizer.get_feature_names_out())[top_idx]\n",
        "    top_weights = coefs[top_idx]\n",
        "\n",
        "    print(f\"{mbti_type}:\")\n",
        "    for feature, weight in zip(top_features, top_weights):\n",
        "        print(f\"  {feature}: {weight:.3f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N8WT139WyWS"
      },
      "source": [
        "TF-IDF analysis highlights the words most distinctive to each MBTI type. Self-references (e.g., ‘enfp’, ‘intj’) dominate, as expected in a dataset labeled by personality. Beyond labels, lexical patterns reflect type tendencies: ENFPs emphasize relational and positive words (‘friend’, ‘love’, ‘excite’), INTJs focus on cognitive or planning related terms (‘te’, ‘plan’, ‘efficient’), and ISTPs/ESTPs feature more concrete or action oriented words (‘se’, ‘military’, ‘fix’). This preliminary observation suggests linguistic markers could differentiate personality types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d45d2904"
      },
      "source": [
        "## 5. Visualization and Insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YDdLrjyDmOX"
      },
      "source": [
        "### MBTI Type Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD4KUPEt5TfW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "sns.countplot(data=data, x='type', order=data['type'].value_counts().index, palette=\"viridis\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Distribution of MBTI Types\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"MBTI Type\")\n",
        "plt.ylabel(\"Number of Posts\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFlqjfNWeXvu"
      },
      "source": [
        "## Dataset Overview: MBTI Posts Distribution\n",
        "\n",
        "The dataset is heavily dominated by the four **Introverted-Intuitive (IN) types**:\n",
        "\n",
        "| Type  | Approx. Posts |\n",
        "|-------|---------------|\n",
        "| INTP  | ~25,000       |\n",
        "| INTJ  | ~22,500       |\n",
        "| INFJ  | ~15,000       |\n",
        "| INFP  | ~12,000       |\n",
        "\n",
        "Other types, particularly **Extraverted-Sensing (ES)** types like ESFP, ESTP, and ESFJ, have very few posts (some < 500).  \n",
        "\n",
        "### Observations\n",
        "1. **Platform bias**: Data is sourced from websites like PersonalityCafe and Reddit, which attract users interested in **in depth discussions** and **anonymous interactions**. This skews the dataset toward **introverted-intuitive types**.\n",
        "2. **Behavioral trends**: Extraverted-sensing types are more likely to use platforms like Instagram and Facebook, which emphasize **visual content, sensory experiences, and non-anonymous interactions**.\n",
        "3. **Impact on modeling**: This imbalance can affect **model accuracy and fairness**, as predictions may overrepresent IN types while underrepresenting ES types.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXaKfloaDpsf"
      },
      "source": [
        "### MBTI Dimension Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3h74t4dDtEg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Map binary to descriptive labels\n",
        "data_plot = data.copy()\n",
        "data_plot[\"I_E_label\"] = data_plot[\"I_E\"].map({1:\"Introvert\", 0:\"Extrovert\"})\n",
        "data_plot[\"S_N_label\"] = data_plot[\"S_N\"].map({1:\"Sensing\", 0:\"Intuition\"})\n",
        "data_plot[\"T_F_label\"] = data_plot[\"T_F\"].map({1:\"Thinking\", 0:\"Feeling\"})\n",
        "data_plot[\"J_P_label\"] = data_plot[\"J_P\"].map({1:\"Judging\", 0:\"Perceiving\"})\n",
        "\n",
        "# Create a 2x2 grid of readable plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14,10))\n",
        "sns.countplot(data=data_plot, x=\"I_E_label\", ax=axes[0,0], palette=\"pastel\")\n",
        "axes[0,0].set_title(\"Introvert vs Extrovert\")\n",
        "\n",
        "sns.countplot(data=data_plot, x=\"S_N_label\", ax=axes[0,1], palette=\"pastel\")\n",
        "axes[0,1].set_title(\"Sensing vs Intuition\")\n",
        "\n",
        "sns.countplot(data=data_plot, x=\"T_F_label\", ax=axes[1,0], palette=\"pastel\")\n",
        "axes[1,0].set_title(\"Thinking vs Feeling\")\n",
        "\n",
        "sns.countplot(data=data_plot, x=\"J_P_label\", ax=axes[1,1], palette=\"pastel\")\n",
        "axes[1,1].set_title(\"Judging vs Perceiving\")\n",
        "\n",
        "# Add percentage labels on top of bars\n",
        "for ax in axes.flatten():\n",
        "    total = len(data_plot)\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        ax.text(p.get_x() + p.get_width()/2, height + 50, f\"{height/total:.1%}\",\n",
        "                ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McdMsEu5gYd4"
      },
      "source": [
        "### 1. Introvert vs Extrovert (I/E)\n",
        "- **Introverts**: 76.1%  \n",
        "- **Extroverts**: 23.9%  \n",
        "\n",
        "The dataset is strongly **I-heavy**, reflecting its origins in long form online communities that naturally attract introverts. Linguistic patterns are therefore likely to be **introspective, analytical, and self focused**.\n",
        "\n",
        "### 2. Sensing vs Intuition (S/N)\n",
        "- **Intuition**: 91.3%  \n",
        "- **Sensing**: 8.7%  \n",
        "\n",
        "Extremely **N-dominant**, making S-type patterns underrepresented and less reliable. Most extracted linguistic features reflect **intuitive-style writing**, so ML models will likely struggle to detect S-types, resulting in the **lowest recall** along this dimension.\n",
        "\n",
        "### 3. Thinking vs Feeling (T/F)\n",
        "- **Thinking**: 65.2%  \n",
        "- **Feeling**: 34.8%  \n",
        "\n",
        "This is the **most balanced dimension**, with enough T and F posts to detect meaningful differences. Models should perform best here. We can analyze:\n",
        "- **Analytical vs. emotional vocabulary**  \n",
        "- **Argumentative vs. experiential phrasing**  \n",
        "- **Logical vs. empathetic sentence structures**\n",
        "\n",
        "### 4. Judging vs Perceiving (J/P)\n",
        "- **Perceiving**: 58.1%  \n",
        "- **Judging**: 41.9%  \n",
        "\n",
        "Slightly skewed but not extreme. J/P linguistic differences remain **reliable for analysis**. P dominance aligns with the N/T dominance, reflecting that **NT and NP types dominate the dataset**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlV5W6KWDvCu"
      },
      "source": [
        "### N-gram Frequency Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uxe_UICjDxdg"
      },
      "outputs": [],
      "source": [
        "# Create subplots for all 16 types\n",
        "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, mbti_type in enumerate(sorted(ngram_analysis.keys())):\n",
        "    top_bigrams = ngram_analysis[mbti_type]['bigrams']\n",
        "    if len(top_bigrams) > 0:\n",
        "        words, counts = zip(*top_bigrams[:5])\n",
        "        axes[idx].barh(list(words), list(counts), color=plt.cm.magma(idx/16))\n",
        "        axes[idx].set_title(mbti_type, fontweight='bold', fontsize=12)\n",
        "        axes[idx].invert_yaxis()\n",
        "        axes[idx].set_xlabel(\"Frequency\", fontsize=9)\n",
        "    else:\n",
        "        axes[idx].text(0.5, 0.5, 'No data', ha='center', va='center')\n",
        "        axes[idx].set_title(mbti_type, fontweight='bold')\n",
        "\n",
        "plt.suptitle(\"Top 5 Bigrams by MBTI Type\", fontsize=18, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16xStmo2jdWx"
      },
      "source": [
        "Across nearly all 16 MBTI types, **“feel like”** dominates as the most frequent bigram, except for **ESTP**.  \n",
        "\n",
        "- **“Feel like” is not an emotional expression** but a **discourse marker**.  \n",
        "  It often signals:\n",
        "  - **Uncertainty:** “I feel like this means…”\n",
        "  - **Opinion introduction:** “I feel like people forget…”  \n",
        "- Its reflects **casual, introspective online communication**, typical of the MBTI dataset community (Reddit and PersonalityCafe).  \n",
        "- Because it appears so frequently across all types, **“feel like” is not a discriminative linguistic feature** for MBTI classification.\n",
        "\n",
        "### Outlier: ESTP\n",
        "- **ESTP** stands out with bigrams such as **“lol lol”**, indicating **more reactive and informal language**.  \n",
        "- This aligns with typical personality descriptions of ESTPs as **action oriented** and **less introspective** compared to IN-dominant types.\n",
        "\n",
        "### Additional Anomaly: ESTJ\n",
        "- An unexpected pattern appears in **ESTJ** bigrams, where **“det er”** (Danish/Norwegian for *“it is”*) ranks among the most frequent.  \n",
        "- This likely reflects **multilingual posts** or **sample-size sensitivity**, as ESTJ has relatively few entries.  \n",
        "- Since bigram counts in smaller classes can be **skewed by a handful of outlier posts**, this highlights the need for **language filtering and careful dataset cleaning** before interpreting linguistic patterns.\n",
        "\n",
        "\n",
        "Other common bigrams across types include **“sound like,” “look like,”** and **“make sense.”**  \n",
        "These reflect a shared linguistic culture centered on **hedging** and **sensory comparison**, suggesting that stylistic norms, rather than personality, heavily shape expression within this dataset.\n",
        "\n",
        "### Implications\n",
        "- Simple **n-gram features** provide limited discriminative power for personality detection.  \n",
        "- **Advanced linguistic features** are needed for more reliable classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3iW5Fm4DzOG"
      },
      "source": [
        "### POS Ratio Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tk5xHxuD0zc"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(pos_summary, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Average POS Ratios by MBTI Type\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"POS Category\", fontweight='bold')\n",
        "plt.ylabel(\"MBTI Type\", fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z22YHCp1k6Z2"
      },
      "source": [
        "POS ratios remain **remarkably consistent** across all MBTI types, showing minimal linguistic variation.\n",
        "\n",
        "| POS Tag | Approx. Range |\n",
        "|----------|----------------|\n",
        "| NOUN | 0.46–0.55 |\n",
        "| VERB | 0.21–0.25 |\n",
        "| ADJ | 0.14–0.16 |\n",
        "| ADV | 0.09–0.11 |\n",
        "| PRON | ~0.02 |\n",
        "\n",
        "- The **narrow range** across all types suggests that **POS distribution is largely constant**, indicating minimal stylistic difference at this grammatical level.\n",
        "- **ESTJ** shows a slightly higher **noun ratio (~0.55)**, possibly reflecting a more **object-oriented or concrete linguistic focus**, which aligns with its personality profile.  \n",
        "  However, this finding should be **interpreted cautiously** due to small sample size and the **earlier anomaly** observed in the ESTJ data.\n",
        "- **Pronoun ratios** are very low (<0.02) across all types, not due to personality differences, but because **stopword removal** eliminated most pronoun tokens. As a result, pronouns **cannot be meaningfully compared** between types.\n",
        "\n",
        "**POS ratios** do not vary significantly among personality types, they are **unlikely to be strong predictive features** for MBTI classification.  \n",
        "Future models will require **semantic, lexical, or contextual features** to capture more meaningful distinctions in linguistic behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXFceP5bPUH8"
      },
      "source": [
        "### TF-IDF Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh9MZhK1PTzB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(20, 18))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, mbti_type in enumerate(clf.classes_):\n",
        "    coefs = clf.coef_[idx]\n",
        "    top_idx = np.argsort(coefs)[-10:]\n",
        "    top_features = np.array(vectorizer.get_feature_names_out())[top_idx]\n",
        "    top_weights = coefs[top_idx]\n",
        "\n",
        "    axes[idx].barh(top_features, top_weights, color=plt.cm.coolwarm(idx/16))\n",
        "    axes[idx].set_title(mbti_type, fontweight='bold', fontsize=11)\n",
        "    axes[idx].invert_yaxis()\n",
        "    axes[idx].set_xlabel(\"Coefficient\", fontsize=9)\n",
        "    axes[idx].tick_params(axis='y', labelsize=8)\n",
        "\n",
        "plt.suptitle(\"Top 10 TF-IDF Features by MBTI Type\", fontsize=18, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_J0mWyaoFYS"
      },
      "source": [
        "The visualization above preents the **top 10 TF-IDF features** for each MBTI type.  \n",
        "\n",
        "Each bar indicates the words most characteristic of that type’s writing within the dataset.\n",
        "\n",
        "- **ENFP**: Terms such as *“excite,” “friend,” “adventure,”* and *“dude”* reflect an **expressive and socially engaged language style**, consistent with their imaginative and outward  oriented traits.  \n",
        "- **ENTP**: High occurrence of **casual and profane language** suggests a **spontaneous and informal communication pattern**, typical of discussion driven or debate heavy exchanges.  \n",
        "- **INTP**: Frequent words like *“think,” “meme,”* and *“philosophy”* indicate **introspective and conceptual writing**, supporting their analytical and reflective tendencies.  \n",
        "- **INTJ**: Keywords *“arrogant,” “plan,”* and *“efficient”* emphasize a **goal oriented, structured, and evaluative tone**, aligning with their reputation for systematic reasoning.  \n",
        "- **INFJ**: The phrase *“door slam”* notably appears among top features, a phrase often used within MBTI communities to describe emotional boundary-setting, reflecting **self-protective or decisive language use**.  \n",
        "- **INFP**: Words like *“dream,” “love,” “beautiful,”* and *“daydream”* demonstrate **emotionally charged and imaginative vocabulary**, reinforcing their association with creative expression.  \n",
        "- **ISFJ**: Common terms such as *“friend,” “quiet,”* and *“boyfriend”* point to **relationship oriented and reserved language**, indicating a focus on personal rather than abstract themes.  \n",
        "- **ESTP**: Bigrams like *“lol,” “haha,”* and *“yeah”* reflect **reactive and conversational writing**, matching their action-focused, socially dynamic tendencies.  \n",
        "- **ESTJ**: Terms such as *“officer,” “germany,”* and *“police”* suggest **structured and factual content**, though interpretation should remain cautious due to **limited sample size** and **previous multilingual anomalies** (e.g., *“det er”*).  \n",
        "\n",
        "\n",
        "TF-IDF captures **distinct lexical markers** for larger classes (especially INFP, ENFP, INTP), while smaller classes are more **susceptible to topic or language bias**.  \n",
        "\n",
        "\n",
        "Personality differentiation appears more **semantic than structural** tone, vocabulary, and abstraction level distinguish types more than syntactic or POS features.\n",
        "\n",
        "\n",
        "TF-IDF provides clear interpretability and highlights personality linked lexical tendencies.  \n",
        "However, due to overlapping vocabulary and community specific slang, **semantic and contextual modeling** would be required for stronger classification performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54a12374"
      },
      "source": [
        "## 6. Ethical Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91_1LKr9-_xG"
      },
      "source": [
        "**Data Privacy:** All posts are publicly available and anonymized.\n",
        "\n",
        "**Bias Awareness:** Overrepresentation of certain types (e.g., INFP, INTP) may affect balance.\n",
        "\n",
        "**Language & Cultural Bias:** Dataset is English only and Reddit centric.\n",
        "\n",
        "**Responsible Interpretation:** Avoid generalizing results to real world psychological profiling.\n",
        "\n",
        "Ethical handling ensures that insights remain educational, non diagnostic, and respect user privacy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c0aac24"
      },
      "source": [
        "## 7. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQjGzOLrS-4y"
      },
      "source": [
        "The linguistic exploration of the MBTI text dataset reveals a strong demographic and skew toward introverted–intuitive types, particularly INTP, INTJ, INFJ, and INFP. This reflects the nature of the data sources platforms, which is as Reddit and PersonalityCafe. It favor users who engage in introspective and abstract self expression discussions. The dataset does not represent the full MBTI spectrum equally, which limits the generalizability of linguistic findings, especially for underrepresented sensing types.\n",
        "\n",
        "Across the four MBTI dimensions, the S/N axis displays the most extreme imbalance (91% Intuitive), followed by I/E (76% Introverted). The T/F and J/P dimensions remain relatively balanced, and therefore more suitable for linguistic modeling. Any personality based classification task should account for these imbalances through resampling, class weighting, and targeted evaluation metrics.\n",
        "\n",
        "At the linguistic level, n-gram and POS analyses indicate that surface level features, such as word frequency or POS distribution, vary minimally across types. Common phrases such as “feel like” dominate nearly all categories, suggesting a shared discourse style rooted in online communication norms rather than personality. Anomalies such as the ESTJ bigram “det er” underscore the importance of language filtering and sample size sensitivity in interpreting results. Similarly, POS ratios remain nearly constant (e.g. NOUN ≈ 0.46–0.55, VERB ≈ 0.21–0.25), confirming that grammatical structure is not a big differentiator among types.\n",
        "\n",
        "In contrast, TF-IDF analysis highlights clear lexical distinctions at the semantic level. Terms associated with abstract reasoning (INTP, INTJ), emotional or imaginative expression (INFP, ENFP), and interpersonal focus (ISFJ, ESFJ) align with personality based expectations. These findings suggest that meaningful differentiation emerges from semantic content rather than syntactic form.\n",
        "\n",
        "Overall, while the dataset supports broad associations between language and personality, its imbalance and community specific bias constrains the predictive reliability. Future work should instead prioritize balanced data collection, multilingual filtering, and the inclusion of context aware linguistic features to capture deeper psychological and semantic patterns beyond surface level text statistics."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgyJlIl5Fj8gOXzepFvFwW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}